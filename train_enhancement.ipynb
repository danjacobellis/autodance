{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a38491-f887-4b7c-953c-09520684b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n",
      "env: CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'k_diffusion.models'; 'k_diffusion' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pil_to_tensor, to_pil_image\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleNamespace\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mk_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transformer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     ImageTransformerDenoiserModelV2,\n\u001b[1;32m     14\u001b[0m     GlobalAttentionSpec,\n\u001b[1;32m     15\u001b[0m     NeighborhoodAttentionSpec\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastprogress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m master_bar, progress_bar\n",
      "File \u001b[0;32m~/danjacobellis/autodance/k_diffusion.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pil_to_tensor, to_pil_image\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleNamespace\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mk_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transformer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     ImageTransformerDenoiserModelV2,\n\u001b[1;32m     17\u001b[0m     GlobalAttentionSpec,\n\u001b[1;32m     18\u001b[0m     NeighborhoodAttentionSpec\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastprogress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m master_bar, progress_bar\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'k_diffusion.models'; 'k_diffusion' is not a package"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n",
    "device=\"cuda:0\"\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from types import SimpleNamespace\n",
    "from k_diffusion.models.image_transformer_v2 import (\n",
    "    ImageTransformerDenoiserModelV2,\n",
    "    GlobalAttentionSpec,\n",
    "    NeighborhoodAttentionSpec\n",
    ")\n",
    "from collections import namedtuple\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from timm.optim import Mars\n",
    "\n",
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_512_f16c12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685fb62-cedf-4247-bf4e-34ab5fc0157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.lr_pow = 2\n",
    "config.batch_size = 32\n",
    "config.max_lr = (64/config.batch_size)*1e-3\n",
    "config.min_lr = config.max_lr / 1e3\n",
    "config.num_workers = 32\n",
    "config.epochs = 2\n",
    "config.total_steps = config.epochs * (dataset['train'].num_rows // config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc80f77-5172-4f00-aa16-23f9831596e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x1 = torch.cat([\n",
    "        pil_to_tensor(sample['image']).unsqueeze(0)\n",
    "        for sample in batch\n",
    "    ]).to(torch.float)/127.5 - 1.0\n",
    "    x2 = torch.cat([\n",
    "        pil_to_tensor(sample['conditioning_image']).unsqueeze(0)\n",
    "        for sample in batch\n",
    "    ]).to(torch.float)/127.5 - 1.0\n",
    "    return x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcb130-b8ba-4a47-984b-1b5703bec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "LevelSpec = namedtuple('LevelSpec', ['width', 'depth', 'd_ff', 'self_attn', 'dropout'])\n",
    "MappingSpec = namedtuple('MappingSpec', ['depth', 'width', 'd_ff', 'dropout'])\n",
    "\n",
    "levels = [\n",
    "    LevelSpec(width=128, depth=2, d_ff=512, self_attn=NeighborhoodAttentionSpec(d_head=64, kernel_size=7), dropout=0.1),\n",
    "    LevelSpec(width=256, depth=2, d_ff=1024, self_attn=NeighborhoodAttentionSpec(d_head=64, kernel_size=7), dropout=0.1),\n",
    "    LevelSpec(width=512, depth=2, d_ff=2048, self_attn=NeighborhoodAttentionSpec(d_head=64, kernel_size=7), dropout=0.1),\n",
    "    LevelSpec(width=1024, depth=4, d_ff=4096, self_attn=GlobalAttentionSpec(d_head=64), dropout=0.1)\n",
    "]\n",
    "mapping = MappingSpec(depth=4, width=512, d_ff=2048, dropout=0.1)\n",
    "\n",
    "# Initialize the model\n",
    "model = ImageTransformerDenoiserModelV2(\n",
    "    levels=levels,\n",
    "    mapping=mapping,\n",
    "    in_channels=6,         # 3 channels for noisy image + 3 for lossy reconstruction\n",
    "    out_channels=3,        # Output is the predicted clean RGB image\n",
    "    patch_size=(4,4),      # Patch size for tokenization\n",
    "    num_classes=0,         # No class conditioning\n",
    "    mapping_cond_dim=0     # No additional mapping conditioning\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed4fc3-fea9-42eb-b315-8c95c0bd875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_sigma(t, eta=0.5, log_snr_min=-15, log_snr_max=15):\n",
    "    log_snr = -2 * (torch.log(torch.tan(math.pi * t / 2)) + math.log(eta))\n",
    "    log_snr = torch.clamp(log_snr, log_snr_min, log_snr_max)\n",
    "    alpha_sq = torch.sigmoid(log_snr)\n",
    "    sigma_sq = torch.sigmoid(-log_snr)\n",
    "    return torch.sqrt(alpha_sq), torch.sqrt(sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f185d11-937f-4956-9ba0-a15064063935",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Mars(model.parameters(), lr=1.0, caution=True)\n",
    "\n",
    "def rc_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return (config.max_lr - config.min_lr) * (1 - ((np.cos(np.pi*t))**(2*config.lr_pow))) + config.min_lr\n",
    "\n",
    "schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda i_step: rc_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bac9e-b68d-4b7b-910d-f5cf6f31b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "mb = master_bar(range(config.epochs))\n",
    "losses = []\n",
    "\n",
    "global_step = 0\n",
    "model.train()\n",
    "for i_epoch in mb:\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "            dataset['train'],\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            collate_fn = collate_fn\n",
    "        )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, (x, x_lossy) in enumerate(pb):\n",
    "        x = x.to(device)\n",
    "        x_lossy = x_lossy.to(device)\n",
    "        batch_size = x.shape[0]\n",
    "        t = torch.rand(batch_size, device=device) * (1.0 - 1e-3) + 1e-3\n",
    "        alpha_t, sigma_t = get_alpha_sigma(t)\n",
    "        epsilon = torch.randn_like(x)\n",
    "        x_noisy = alpha_t.view(-1, 1, 1, 1) * x + sigma_t.view(-1, 1, 1, 1) * epsilon\n",
    "        x_input = torch.cat([x_noisy, x_lossy], dim=1)  # [batch, 6, 512, 512]\n",
    "        x_pred = model(x_input, sigma_t)  # [batch, 3, 512, 512]\n",
    "        loss = F.mse_loss(x_pred, x)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        schedule.step()\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        pb.comment = (f\"PSNR: {-10*losses[-1]+6.02:.3g}, LR: {learning_rates[-1]:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecbd0a-92cb-4d22-b438-a1fe6b7bcb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
