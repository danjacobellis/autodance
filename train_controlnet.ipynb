{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7e2ba9-819e-44af-a038-d518e3ee690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device=\"cuda:0\"\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from IPython.display import display\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FluxTransformer2DModel,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    FluxControlNetModel,\n",
    ")\n",
    "from diffusers.pipelines.flux.pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "from diffusers.training_utils import free_memory\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import AutoTokenizer, CLIPTextModel, T5EncoderModel\n",
    "from huggingface_hub import create_repo, upload_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29e0849-b46e-4353-899d-f22ab24ec118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748b7aee5fb3410cb5b0ea8e3ced32fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484e7a9243bf48d894bafe6b9bb8b0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86031deaa0f4de6b437e34bf2fb5263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset(\"danjacobellis/LSDIR_512_f16c12\", split=\"train\")\n",
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_540\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26466ee1-d34a-4044-a715-975ac849e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.pretrained_model_name_or_path = \"black-forest-labs/FLUX.1-dev\"\n",
    "config.output_dir = \"./flux_controlnet_codec_enhancer\"\n",
    "config.resolution = 512\n",
    "config.batch_size = 2\n",
    "config.num_double_layers = 4\n",
    "config.num_single_layers = 0\n",
    "config.learning_rate = 1e-5\n",
    "config.max_train_steps = 15000\n",
    "config.checkpointing_steps = 200\n",
    "config.validation_steps = 100\n",
    "config.gradient_accumulation_steps = 4\n",
    "config.seed = 42\n",
    "config.push_to_hub = True\n",
    "config.hub_model_id = \"danjacobellis/FLUX.1-controlnet-codec-enhancer\"\n",
    "config.num_validation_images = 1\n",
    "config.validation_prompts = [\n",
    "    \"blurry image\",\n",
    "]\n",
    "config.validation_images = [\n",
    "    \"compressed.png\",\n",
    "]\n",
    "config.total_steps = config.max_train_steps * config.gradient_accumulation_steps\n",
    "config.tracker_project_name = \"flux_controlnet_codec_enhancer\"\n",
    "config.num_train_epochs = math.ceil(config.max_train_steps / (dataset.num_rows // config.batch_size))\n",
    "config.guidance_scale = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696413e9-37db-4ed2-b793-f19624c09269",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed08260-bec0-4011-b7ed-686c49373757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587213b0dde44212b0f92e459071229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee888ae6d6da456ba01514a6d0a09ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer_2\"\n",
    ")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "text_encoder_two = T5EncoderModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b006c7b7-b814-46e9-8b16-b38e58ade9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f61a5be2af44c44ad9821d004278426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae936fb75ae48459aa03b136656fdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "flux_transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"transformer\"\n",
    ")\n",
    "flux_controlnet = FluxControlNetModel.from_transformer(\n",
    "    flux_transformer,\n",
    "    attention_head_dim=flux_transformer.config[\"attention_head_dim\"],\n",
    "    num_attention_heads=flux_transformer.config[\"num_attention_heads\"],\n",
    "    num_layers=config.num_double_layers,\n",
    "    num_single_layers=config.num_single_layers,\n",
    ")\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")\n",
    "noise_scheduler_copy = noise_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e14762-ab74-4131-a7e5-e6ed5984ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.requires_grad_(False)\n",
    "flux_transformer.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "flux_controlnet.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc0e554-f65e-4727-bf04-471d66ecd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device, dtype=torch.float32)\n",
    "flux_transformer.to(device, dtype=torch.float32)\n",
    "text_encoder_one.to(device, dtype=torch.float32)\n",
    "text_encoder_two.to(device, dtype=torch.float32)\n",
    "flux_controlnet.to(device, dtype=torch.float32)\n",
    "\n",
    "# Initialize pipeline for utility functions\n",
    "pipeline = FluxControlNetPipeline(\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder_one,\n",
    "    tokenizer=tokenizer_one,\n",
    "    text_encoder_2=text_encoder_two,\n",
    "    tokenizer_2=tokenizer_two,\n",
    "    transformer=flux_transformer,\n",
    "    controlnet=flux_controlnet,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8378663e-a09d-48c5-b71d-05d1f33e56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    flux_controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "def lr_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return config.learning_rate * (1 - ((np.cos(np.pi * t)) ** 2))\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda i_step: lr_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a3180b-1936-424b-85a3-9a850451c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(config.resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec3b706-efab-4c45-af7f-adbaa3eb0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    image = image_transforms(example[\"image\"])\n",
    "    return {\n",
    "        \"pixel_values\": image,\n",
    "        \"conditioning_pixel_values\": image,\n",
    "        \"caption\": \"high-quality enhanced image\"  # Generic caption as LSDIR lacks captions\n",
    "    }\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_dataset = dataset.select(range(100)).map(preprocess, remove_columns=[\"image\"])\n",
    "    train_dataset.set_format(\"torch\")\n",
    "\n",
    "def compute_embeddings(batch):\n",
    "    captions = batch[\"caption\"]\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(captions, prompt_2=captions)\n",
    "    text_ids = text_ids.unsqueeze(0).expand(prompt_embeds.shape[0], -1, -1)  # [bs, 512, 3]\n",
    "    return {\n",
    "        \"prompt_embeds\": prompt_embeds,\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "        \"text_ids\": text_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68413019-2826-402e-9717-dd0359bee4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413f5d018a7f4d36a57daeeb7986ac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    compute_embeddings, remove_columns=[\"caption\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a429fe46-02bc-46f4-9616-a51b9bb333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch]).to(torch.float32)\n",
    "    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in batch]).to(torch.float32)\n",
    "    prompt_ids = torch.stack([example[\"prompt_embeds\"] for example in batch]).to(torch.float32)\n",
    "    pooled_prompt_embeds = torch.stack([example[\"pooled_prompt_embeds\"] for example in batch]).to(torch.float32)\n",
    "    text_ids = torch.stack([example[\"text_ids\"] for example in batch]).to(torch.float32)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"unet_added_conditions\": {\"pooled_prompt_embeds\": pooled_prompt_embeds, \"time_ids\": text_ids},\n",
    "    }\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78fa586c-03ed-4533-ac9d-4e3c91890860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(step):\n",
    "    flux_controlnet.eval()\n",
    "    pipeline.controlnet = flux_controlnet\n",
    "    generator = torch.Generator(device=device).manual_seed(config.seed)\n",
    "    image_logs = []\n",
    "\n",
    "    for v_prompt, v_image_path in zip(config.validation_prompts, config.validation_images):\n",
    "        v_image = Image.open(v_image_path).convert(\"RGB\").resize((config.resolution, config.resolution))\n",
    "        v_image_tensor = conditioning_transforms(v_image).to(device, dtype=torch.float32)\n",
    "        images = []\n",
    "        prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(v_prompt, prompt_2=v_prompt)\n",
    "        for _ in range(config.num_validation_images):\n",
    "            with torch.no_grad():\n",
    "                image = pipeline(\n",
    "                    prompt_embeds=prompt_embeds,\n",
    "                    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                    control_image=v_image_tensor.unsqueeze(0),\n",
    "                    num_inference_steps=28,\n",
    "                    controlnet_conditioning_scale=0.7,\n",
    "                    guidance_scale=3.5,\n",
    "                    generator=generator,\n",
    "                ).images[0]\n",
    "            images.append(image)\n",
    "        image_logs.append({\"validation_image\": v_image, \"images\": images, \"validation_prompt\": v_prompt})\n",
    "\n",
    "    # Display validation results\n",
    "    for log in image_logs:\n",
    "        grid = make_image_grid([log[\"validation_image\"]] + log[\"images\"], 1, config.num_validation_images + 1)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.imshow(np.array(grid))\n",
    "        plt.title(f\"Step {step}: {log['validation_prompt']}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    flux_controlnet.train()\n",
    "    free_memory()\n",
    "    return image_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe21f78-4884-4b68-b5ba-6433aa0684f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigma function\n",
    "def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):\n",
    "    sigmas = noise_scheduler.sigmas.to(device=device, dtype=dtype)\n",
    "    schedule_timesteps = noise_scheduler.timesteps.to(device)\n",
    "    timesteps = timesteps.to(device)\n",
    "    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    while len(sigma.shape) < n_dim:\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d09020-75a9-465c-9ef5-9b5e1937ed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/7500 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/50 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m\n\u001b[1;32m     58\u001b[0m guidance_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m     59\u001b[0m     (noisy_model_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],),\n\u001b[1;32m     60\u001b[0m     config\u001b[38;5;241m.\u001b[39mguidance_scale,  \u001b[38;5;66;03m# e.g., 3.5 or your desired scale\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     62\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnoisy_model_input\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# ControlNet forward\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m controlnet_block_samples, controlnet_single_block_samples \u001b[38;5;241m=\u001b[39m \u001b[43mflux_controlnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet_added_conditions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooled_prompt_embeds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet_added_conditions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_image_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Flux transformer forward\u001b[39;00m\n\u001b[1;32m     79\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m flux_transformer(\n\u001b[1;32m     80\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mnoisy_model_input,\n\u001b[1;32m     81\u001b[0m     timestep\u001b[38;5;241m=\u001b[39mtimesteps \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/controlnets/controlnet_flux.py:338\u001b[0m, in \u001b[0;36mFluxControlNetModel.forward\u001b[0;34m(self, hidden_states, controlnet_cond, controlnet_mode, conditioning_scale, encoder_hidden_states, pooled_projections, timestep, img_ids, txt_ids, guidance, joint_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    329\u001b[0m         encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    330\u001b[0m             block,\n\u001b[1;32m    331\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m             image_rotary_emb,\n\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m         encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     block_samples \u001b[38;5;241m=\u001b[39m block_samples \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    346\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([encoder_hidden_states, hidden_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/transformers/transformer_flux.py:144\u001b[0m, in \u001b[0;36mFluxTransformerBlock.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, temb, image_rotary_emb, joint_attention_kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     joint_attention_kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 144\u001b[0m     norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1_context(\n\u001b[1;32m    147\u001b[0m         encoder_hidden_states, emb\u001b[38;5;241m=\u001b[39mtemb\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m     joint_attention_kwargs \u001b[38;5;241m=\u001b[39m joint_attention_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/normalization.py:169\u001b[0m, in \u001b[0;36mAdaLayerNormZero.forward\u001b[0;34m(self, x, timestep, class_labels, hidden_dtype, emb)\u001b[0m\n\u001b[1;32m    167\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(timestep, class_labels, hidden_dtype\u001b[38;5;241m=\u001b[39mhidden_dtype)\n\u001b[1;32m    168\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msilu(emb))\n\u001b[0;32m--> 169\u001b[0m shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m6\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m scale_msa[:, \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m+\u001b[39m shift_msa[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, gate_msa, shift_mlp, scale_mlp, gate_mlp\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 2)"
     ]
    }
   ],
   "source": [
    "mb = master_bar(range(config.max_train_steps // config.batch_size))\n",
    "learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "losses = []\n",
    "global_step = 0\n",
    "flux_controlnet.train()\n",
    "\n",
    "for i_step in mb:\n",
    "    pb = progress_bar(dataloader, parent=mb)\n",
    "    for i_batch, batch in enumerate(pb):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        conditioning_pixel_values = batch[\"conditioning_pixel_values\"].to(device)\n",
    "        prompt_ids = batch[\"prompt_ids\"].to(device)\n",
    "        unet_added_conditions = {\n",
    "            \"pooled_prompt_embeds\": batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device),\n",
    "            \"time_ids\": batch[\"unet_added_conditions\"][\"time_ids\"].to(device),\n",
    "        }\n",
    "\n",
    "        # Encode images to latents\n",
    "        with torch.no_grad():\n",
    "            pixel_latents_tmp = vae.encode(pixel_values).latent_dist.sample()\n",
    "            pixel_latents_tmp = (pixel_latents_tmp - vae.config.shift_factor) * vae.config.scaling_factor\n",
    "            pixel_latents = pipeline._pack_latents(\n",
    "                pixel_latents_tmp,\n",
    "                pixel_values.shape[0],\n",
    "                pixel_latents_tmp.shape[1],\n",
    "                pixel_latents_tmp.shape[2],\n",
    "                pixel_latents_tmp.shape[3],\n",
    "            )\n",
    "\n",
    "            control_latents_tmp = vae.encode(conditioning_pixel_values).latent_dist.sample()\n",
    "            control_latents_tmp = (control_latents_tmp - vae.config.shift_factor) * vae.config.scaling_factor\n",
    "            control_image = pipeline._pack_latents(\n",
    "                control_latents_tmp,\n",
    "                conditioning_pixel_values.shape[0],\n",
    "                control_latents_tmp.shape[1],\n",
    "                control_latents_tmp.shape[2],\n",
    "                control_latents_tmp.shape[3],\n",
    "            )\n",
    "\n",
    "        # Prepare latent image IDs using unpacked latents\n",
    "        latent_image_ids = pipeline._prepare_latent_image_ids(\n",
    "            batch_size=pixel_latents_tmp.shape[0],\n",
    "            height=pixel_latents_tmp.shape[2] // 2,\n",
    "            width=pixel_latents_tmp.shape[3] // 2,\n",
    "            device=device,\n",
    "            dtype=pixel_latents.dtype,\n",
    "        )\n",
    "\n",
    "        # Sample noise and timesteps\n",
    "        noise = torch.randn_like(pixel_latents).to(device)\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (pixel_latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Add noise\n",
    "        sigmas = get_sigmas(timesteps, n_dim=pixel_latents.ndim, dtype=pixel_latents.dtype)\n",
    "        noisy_model_input = (1.0 - sigmas) * pixel_latents + sigmas * noise\n",
    "\n",
    "        # Guidance\n",
    "        guidance_vec = torch.full(\n",
    "            (noisy_model_input.shape[0],),\n",
    "            config.guidance_scale,  # e.g., 3.5 or your desired scale\n",
    "            device=device,\n",
    "            dtype=noisy_model_input.dtype\n",
    "        )\n",
    "\n",
    "        # ControlNet forward\n",
    "        controlnet_block_samples, controlnet_single_block_samples = flux_controlnet(\n",
    "            hidden_states=noisy_model_input,\n",
    "            controlnet_cond=control_image,\n",
    "            timestep=timesteps / 1000,\n",
    "            guidance=guidance_vec,\n",
    "            pooled_projections=unet_added_conditions[\"pooled_prompt_embeds\"],\n",
    "            encoder_hidden_states=prompt_ids,\n",
    "            txt_ids=unet_added_conditions[\"time_ids\"][0],\n",
    "            img_ids=latent_image_ids,\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "        # Flux transformer forward\n",
    "        noise_pred = flux_transformer(\n",
    "            hidden_states=noisy_model_input,\n",
    "            timestep=timesteps / 1000,\n",
    "            guidance=guidance_vec,\n",
    "            pooled_projections=unet_added_conditions[\"pooled_prompt_embeds\"],\n",
    "            encoder_hidden_states=prompt_ids,\n",
    "            controlnet_block_samples=controlnet_block_samples,\n",
    "            controlnet_single_block_samples=controlnet_single_block_samples,\n",
    "            txt_ids=unet_added_conditions[\"time_ids\"],\n",
    "            img_ids=latent_image_ids,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(noise_pred, (noise - pixel_latents), reduction=\"mean\")\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        global_step += 1\n",
    "        pb.comment = f\"Loss: {loss.item():.4f}, LR: {learning_rates[-1]:.2g}\"\n",
    "\n",
    "        # Validation and checkpointing\n",
    "        current_step = i_step * config.batch_size + i_batch\n",
    "        if global_step % config.validation_steps == 0:\n",
    "            log_validation(global_step)\n",
    "        if global_step % config.checkpointing_steps == 0:\n",
    "            torch.save(flux_controlnet.state_dict(), os.path.join(config.output_dir, f\"checkpoint_{global_step}.pth\"))\n",
    "\n",
    "        if global_step >= config.max_train_steps:\n",
    "            break\n",
    "    if global_step >= config.max_train_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c98b7-d15a-47da-b60d-6b104b40cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
