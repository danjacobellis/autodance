{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7e2ba9-819e-44af-a038-d518e3ee690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device=\"cuda:0\"\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from IPython.display import display\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FluxTransformer2DModel,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    FluxControlNetModel,\n",
    ")\n",
    "from diffusers.pipelines.flux.pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "from diffusers.training_utils import free_memory\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import AutoTokenizer, CLIPTextModel, T5EncoderModel\n",
    "from huggingface_hub import create_repo, upload_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29e0849-b46e-4353-899d-f22ab24ec118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab85d60e84b493aabfbec92c09aba0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96df712f86424e14ba958ad8d105a0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f076200acfa47cd88351e88a848effa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset(\"danjacobellis/LSDIR_512_f16c12\", split=\"train\")\n",
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_540\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26466ee1-d34a-4044-a715-975ac849e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.pretrained_model_name_or_path = \"black-forest-labs/FLUX.1-dev\"\n",
    "config.output_dir = \"./flux_controlnet_codec_enhancer\"\n",
    "config.resolution = 512\n",
    "config.batch_size = 2\n",
    "config.num_double_layers = 4\n",
    "config.num_single_layers = 0\n",
    "config.learning_rate = 1e-5\n",
    "config.max_train_steps = 15000\n",
    "config.checkpointing_steps = 200\n",
    "config.validation_steps = 100\n",
    "config.gradient_accumulation_steps = 4\n",
    "config.seed = 42\n",
    "config.push_to_hub = True\n",
    "config.hub_model_id = \"danjacobellis/FLUX.1-controlnet-codec-enhancer\"\n",
    "config.num_validation_images = 1\n",
    "config.validation_prompts = [\n",
    "    \"blurry image\",\n",
    "]\n",
    "config.validation_images = [\n",
    "    \"compressed.png\",\n",
    "]\n",
    "config.total_steps = config.max_train_steps * config.gradient_accumulation_steps\n",
    "config.tracker_project_name = \"flux_controlnet_codec_enhancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696413e9-37db-4ed2-b793-f19624c09269",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed08260-bec0-4011-b7ed-686c49373757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4526fc968c76404c89f473f3a0e9f3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8222cf055f40f1b7125ae658e2ae70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer_2\"\n",
    ")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "text_encoder_two = T5EncoderModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b006c7b7-b814-46e9-8b16-b38e58ade9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0234abfdbb4304a305fae7063dda6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "flux_transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"transformer\"\n",
    ")\n",
    "flux_controlnet = FluxControlNetModel.from_transformer(\n",
    "    flux_transformer,\n",
    "    attention_head_dim=flux_transformer.config[\"attention_head_dim\"],\n",
    "    num_attention_heads=flux_transformer.config[\"num_attention_heads\"],\n",
    "    num_layers=config.num_double_layers,\n",
    "    num_single_layers=config.num_single_layers,\n",
    ")\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e14762-ab74-4131-a7e5-e6ed5984ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.requires_grad_(False)\n",
    "flux_transformer.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "flux_controlnet.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc0e554-f65e-4727-bf04-471d66ecd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device, dtype=torch.float32)\n",
    "flux_transformer.to(device, dtype=torch.float32)\n",
    "text_encoder_one.to(device, dtype=torch.float32)\n",
    "text_encoder_two.to(device, dtype=torch.float32)\n",
    "flux_controlnet.to(device, dtype=torch.float32)\n",
    "\n",
    "# Initialize pipeline for utility functions\n",
    "pipeline = FluxControlNetPipeline(\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder_one,\n",
    "    tokenizer=tokenizer_one,\n",
    "    text_encoder_2=text_encoder_two,\n",
    "    tokenizer_2=tokenizer_two,\n",
    "    transformer=flux_transformer,\n",
    "    controlnet=flux_controlnet,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8378663e-a09d-48c5-b71d-05d1f33e56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    flux_controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "def lr_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return config.learning_rate * (1 - ((np.cos(np.pi * t)) ** 2))\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda i_step: lr_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a3180b-1936-424b-85a3-9a850451c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(config.resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3b706-efab-4c45-af7f-adbaa3eb0421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663a12958e564b6aa253002182ea3b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    image = image_transforms(example[\"image\"])\n",
    "    return {\n",
    "        \"pixel_values\": image,\n",
    "        \"conditioning_pixel_values\": image,\n",
    "        \"caption\": \"high-quality enhanced image\"  # Generic caption as LSDIR lacks captions\n",
    "    }\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_dataset = dataset.map(preprocess, remove_columns=[\"image\"])\n",
    "    train_dataset.set_format(\"torch\")\n",
    "\n",
    "def compute_embeddings(batch):\n",
    "    captions = batch[\"caption\"]\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(captions, prompt_2=captions)\n",
    "    text_ids = text_ids.unsqueeze(0).expand(prompt_embeds.shape[0], -1, -1)  # [bs, 512, 3]\n",
    "    return {\n",
    "        \"prompt_embeds\": prompt_embeds,\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "        \"text_ids\": text_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68413019-2826-402e-9717-dd0359bee4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    compute_embeddings, batched=True, batch_size=50, remove_columns=[\"caption\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429fe46-02bc-46f4-9616-a51b9bb333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch]).to(torch.float32)\n",
    "    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in batch]).to(torch.float32)\n",
    "    prompt_ids = torch.stack([example[\"prompt_embeds\"] for example in batch]).to(torch.float32)\n",
    "    pooled_prompt_embeds = torch.stack([example[\"pooled_prompt_embeds\"] for example in batch]).to(torch.float32)\n",
    "    text_ids = torch.stack([example[\"text_ids\"] for example in batch]).to(torch.float32)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"unet_added_conditions\": {\"pooled_prompt_embeds\": pooled_prompt_embeds, \"time_ids\": text_ids},\n",
    "    }\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa586c-03ed-4533-ac9d-4e3c91890860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(step):\n",
    "    flux_controlnet.eval()\n",
    "    pipeline.controlnet = flux_controlnet\n",
    "    generator = torch.Generator(device=device).manual_seed(config.seed)\n",
    "    image_logs = []\n",
    "\n",
    "    for v_prompt, v_image_path in zip(config.validation_prompts, config.validation_images):\n",
    "        v_image = Image.open(v_image_path).convert(\"RGB\").resize((config.resolution, config.resolution))\n",
    "        v_image_tensor = conditioning_transforms(v_image).to(device, dtype=torch.float32)\n",
    "        images = []\n",
    "        prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(v_prompt, prompt_2=v_prompt)\n",
    "        for _ in range(config.num_validation_images):\n",
    "            with torch.no_grad():\n",
    "                image = pipeline(\n",
    "                    prompt_embeds=prompt_embeds,\n",
    "                    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                    control_image=v_image_tensor.unsqueeze(0),\n",
    "                    num_inference_steps=28,\n",
    "                    controlnet_conditioning_scale=0.7,\n",
    "                    guidance_scale=3.5,\n",
    "                    generator=generator,\n",
    "                ).images[0]\n",
    "            images.append(image)\n",
    "        image_logs.append({\"validation_image\": v_image, \"images\": images, \"validation_prompt\": v_prompt})\n",
    "\n",
    "    # Display validation results\n",
    "    for log in image_logs:\n",
    "        grid = make_image_grid([log[\"validation_image\"]] + log[\"images\"], 1, config.num_validation_images + 1)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.imshow(np.array(grid))\n",
    "        plt.title(f\"Step {step}: {log['validation_prompt']}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    flux_controlnet.train()\n",
    "    free_memory()\n",
    "    return image_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702599b-266f-4e69-af13-766e1a23c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "learning_rates = [optimizer.param_groups[0][\"lr\"]]\n",
    "losses = []\n",
    "mb = master_bar(range(config.max_train_steps // config.gradient_accumulation_steps))\n",
    "\n",
    "for epoch in mb:\n",
    "    pb = progress_bar(dataloader, parent=mb)\n",
    "    for i_batch, batch in enumerate(pb):\n",
    "        if global_step >= config.max_train_steps:\n",
    "            break\n",
    "\n",
    "        # Accumulate gradients\n",
    "        for _ in range(config.gradient_accumulation_steps):\n",
    "            with torch.no_grad():\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)\n",
    "                pixel_latents = FluxControlNetPipeline._pack_latents(\n",
    "                    vae.encode(pixel_values).latent_dist.sample(),\n",
    "                    config.batch_size, 16, config.resolution, config.resolution\n",
    "                )\n",
    "                control_image = FluxControlNetPipeline._pack_latents(\n",
    "                    vae.encode(batch[\"conditioning_pixel_values\"].to(device, dtype=torch.float32)).latent_dist.sample(),\n",
    "                    config.batch_size, 16, config.resolution, config.resolution\n",
    "                )\n",
    "                latent_image_ids = FluxControlNetPipeline._prepare_latent_image_ids(\n",
    "                    config.batch_size, config.resolution // 2, config.resolution // 2, device, torch.float32\n",
    "                )\n",
    "\n",
    "            noise = torch.randn_like(pixel_latents, device=device, dtype=torch.float32)\n",
    "            bsz = pixel_latents.shape[0]\n",
    "            timesteps = torch.rand(bsz, device=device) * noise_scheduler.config.num_train_timesteps\n",
    "            noisy_latents = (1 - timesteps.view(-1, 1, 1)) * pixel_latents + timesteps.view(-1, 1, 1) * noise\n",
    "            guidance_vec = torch.full((bsz,), 3.5, device=device, dtype=torch.float32)\n",
    "\n",
    "            controlnet_block_samples, controlnet_single_block_samples = flux_controlnet(\n",
    "                hidden_states=noisy_latents,\n",
    "                controlnet_cond=control_image,\n",
    "                timestep=timesteps / 1000,\n",
    "                guidance=guidance_vec,\n",
    "                pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device, dtype=torch.float32),\n",
    "                encoder_hidden_states=batch[\"prompt_ids\"].to(device, dtype=torch.float32),\n",
    "                txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(device, dtype=torch.float32),\n",
    "                img_ids=latent_image_ids,\n",
    "                return_dict=False,\n",
    "            )\n",
    "\n",
    "            noise_pred = flux_transformer(\n",
    "                hidden_states=noisy_latents,\n",
    "                timestep=timesteps / 1000,\n",
    "                guidance=guidance_vec,\n",
    "                pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device, dtype=torch.float32),\n",
    "                encoder_hidden_states=batch[\"prompt_ids\"].to(device, dtype=torch.float32),\n",
    "                controlnet_block_samples=controlnet_block_samples,\n",
    "                controlnet_single_block_samples=controlnet_single_block_samples,\n",
    "                txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(device, dtype=torch.float32),\n",
    "                img_ids=latent_image_ids,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            loss = F.mse_loss(noise_pred, (noise - pixel_latents), reduction=\"mean\")\n",
    "            loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        torch.nn.utils.clip_grad_norm_(flux_controlnet.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging\n",
    "        losses.append(loss.item())\n",
    "        learning_rates.append(optimizer.param_groups[0][\"lr\"])\n",
    "        pb.comment = f\"Loss: {losses[-1]:.4f}, LR: {learning_rates[-1]:.2e}\"\n",
    "        global_step += 1\n",
    "\n",
    "        # Checkpointing\n",
    "        if global_step % config.checkpointing_steps == 0:\n",
    "            save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "            flux_controlnet.save_pretrained(save_path)\n",
    "            print(f\"Saved checkpoint to {save_path}\")\n",
    "\n",
    "        # Validation\n",
    "        if global_step % config.validation_steps == 0:\n",
    "            image_logs = log_validation(global_step)\n",
    "\n",
    "    if global_step >= config.max_train_steps:\n",
    "        break\n",
    "\n",
    "# In[8]: Save Final Model\n",
    "flux_controlnet.save_pretrained(config.output_dir)\n",
    "final_image_logs = log_validation(global_step)\n",
    "\n",
    "if config.push_to_hub:\n",
    "    repo_id = create_repo(repo_id=config.hub_model_id, exist_ok=True).repo_id\n",
    "    readme = f\"\"\"\n",
    "# FLUX.1 ControlNet for Codec Enhancement\n",
    "Trained on {config.pretrained_model_name_or_path} to enhance outputs of a neural image codec.\n",
    "Dataset: danjacobellis/LSDIR_512_f16c12\n",
    "\"\"\"\n",
    "    for i, log in enumerate(final_image_logs):\n",
    "        grid = make_image_grid([log[\"validation_image\"]] + log[\"images\"], 1, config.num_validation_images + 1)\n",
    "        grid.save(os.path.join(config.output_dir, f\"example_{i}.png\"))\n",
    "        readme += f\"\\n![Example {i}](example_{i}.png)\\nPrompt: {log['validation_prompt']}\\n\"\n",
    "\n",
    "    with open(os.path.join(config.output_dir, \"README.md\"), \"w\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=config.output_dir,\n",
    "        commit_message=\"Final trained ControlNet\",\n",
    "        ignore_patterns=[\"checkpoint-*\"],\n",
    "    )\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
