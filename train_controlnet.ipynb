{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7e2ba9-819e-44af-a038-d518e3ee690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device=\"cuda:0\"\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from IPython.display import display\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FluxTransformer2DModel,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    FluxControlNetModel,\n",
    ")\n",
    "from diffusers.pipelines.flux.pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "from diffusers.training_utils import free_memory\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import AutoTokenizer, CLIPTextModel, T5EncoderModel\n",
    "from huggingface_hub import create_repo, upload_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29e0849-b46e-4353-899d-f22ab24ec118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2051a8df025e47279397532711b0d684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaf7b292979404dbf78876808737c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2935a0ef8ee4882ab20766ca91cf1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset(\"danjacobellis/LSDIR_512_f16c12\", split=\"train\")\n",
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_540\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26466ee1-d34a-4044-a715-975ac849e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.pretrained_model_name_or_path = \"black-forest-labs/FLUX.1-dev\"\n",
    "config.output_dir = \"./flux_controlnet_codec_enhancer\"\n",
    "config.resolution = 512\n",
    "config.batch_size = 2\n",
    "config.num_double_layers = 4\n",
    "config.num_single_layers = 0\n",
    "config.learning_rate = 1e-5\n",
    "config.max_train_steps = 15000\n",
    "config.checkpointing_steps = 200\n",
    "config.validation_steps = 100\n",
    "config.gradient_accumulation_steps = 4\n",
    "config.seed = 42\n",
    "config.push_to_hub = True\n",
    "config.hub_model_id = \"danjacobellis/FLUX.1-controlnet-codec-enhancer\"\n",
    "config.num_validation_images = 1\n",
    "config.validation_prompts = [\n",
    "    \"blurry image\",\n",
    "]\n",
    "config.validation_images = [\n",
    "    \"compressed.png\",\n",
    "]\n",
    "config.total_steps = config.max_train_steps * config.gradient_accumulation_steps\n",
    "config.tracker_project_name = \"flux_controlnet_codec_enhancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696413e9-37db-4ed2-b793-f19624c09269",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed08260-bec0-4011-b7ed-686c49373757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0e7edefb2a4ec0a6878d92065f8beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdc553c782d4a0498a25e21225294d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer_2\"\n",
    ")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "text_encoder_two = T5EncoderModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b006c7b7-b814-46e9-8b16-b38e58ade9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dae08b0f924e7084aed3a9a91c4649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "flux_transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"transformer\"\n",
    ")\n",
    "flux_controlnet = FluxControlNetModel.from_transformer(\n",
    "    flux_transformer,\n",
    "    attention_head_dim=flux_transformer.config[\"attention_head_dim\"],\n",
    "    num_attention_heads=flux_transformer.config[\"num_attention_heads\"],\n",
    "    num_layers=config.num_double_layers,\n",
    "    num_single_layers=config.num_single_layers,\n",
    ")\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e14762-ab74-4131-a7e5-e6ed5984ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.requires_grad_(False)\n",
    "flux_transformer.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "flux_controlnet.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc0e554-f65e-4727-bf04-471d66ecd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device, dtype=torch.float32)\n",
    "flux_transformer.to(device, dtype=torch.float32)\n",
    "text_encoder_one.to(device, dtype=torch.float32)\n",
    "text_encoder_two.to(device, dtype=torch.float32)\n",
    "flux_controlnet.to(device, dtype=torch.float32)\n",
    "\n",
    "# Initialize pipeline for utility functions\n",
    "pipeline = FluxControlNetPipeline(\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder_one,\n",
    "    tokenizer=tokenizer_one,\n",
    "    text_encoder_2=text_encoder_two,\n",
    "    tokenizer_2=tokenizer_two,\n",
    "    transformer=flux_transformer,\n",
    "    controlnet=flux_controlnet,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8378663e-a09d-48c5-b71d-05d1f33e56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    flux_controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "def lr_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return config.learning_rate * (1 - ((np.cos(np.pi * t)) ** 2))\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda i_step: lr_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a3180b-1936-424b-85a3-9a850451c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(config.resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec3b706-efab-4c45-af7f-adbaa3eb0421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18e67c21d544d969f347169b54e8d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    image = image_transforms(example[\"image\"])\n",
    "    return {\n",
    "        \"pixel_values\": image,\n",
    "        \"conditioning_pixel_values\": image,\n",
    "        \"caption\": \"high-quality enhanced image\"  # Generic caption as LSDIR lacks captions\n",
    "    }\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_dataset = dataset.select(range(100)).map(preprocess, remove_columns=[\"image\"])\n",
    "    train_dataset.set_format(\"torch\")\n",
    "\n",
    "def compute_embeddings(batch):\n",
    "    captions = batch[\"caption\"]\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(captions, prompt_2=captions)\n",
    "    text_ids = text_ids.unsqueeze(0).expand(prompt_embeds.shape[0], -1, -1)  # [bs, 512, 3]\n",
    "    return {\n",
    "        \"prompt_embeds\": prompt_embeds,\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "        \"text_ids\": text_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68413019-2826-402e-9717-dd0359bee4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594f6c619c3d403b87631a7b976dccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    compute_embeddings, remove_columns=[\"caption\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a429fe46-02bc-46f4-9616-a51b9bb333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch]).to(torch.float32)\n",
    "    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in batch]).to(torch.float32)\n",
    "    prompt_ids = torch.stack([example[\"prompt_embeds\"] for example in batch]).to(torch.float32)\n",
    "    pooled_prompt_embeds = torch.stack([example[\"pooled_prompt_embeds\"] for example in batch]).to(torch.float32)\n",
    "    text_ids = torch.stack([example[\"text_ids\"] for example in batch]).to(torch.float32)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"unet_added_conditions\": {\"pooled_prompt_embeds\": pooled_prompt_embeds, \"time_ids\": text_ids},\n",
    "    }\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78fa586c-03ed-4533-ac9d-4e3c91890860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(step):\n",
    "    flux_controlnet.eval()\n",
    "    pipeline.controlnet = flux_controlnet\n",
    "    generator = torch.Generator(device=device).manual_seed(config.seed)\n",
    "    image_logs = []\n",
    "\n",
    "    for v_prompt, v_image_path in zip(config.validation_prompts, config.validation_images):\n",
    "        v_image = Image.open(v_image_path).convert(\"RGB\").resize((config.resolution, config.resolution))\n",
    "        v_image_tensor = conditioning_transforms(v_image).to(device, dtype=torch.float32)\n",
    "        images = []\n",
    "        prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(v_prompt, prompt_2=v_prompt)\n",
    "        for _ in range(config.num_validation_images):\n",
    "            with torch.no_grad():\n",
    "                image = pipeline(\n",
    "                    prompt_embeds=prompt_embeds,\n",
    "                    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                    control_image=v_image_tensor.unsqueeze(0),\n",
    "                    num_inference_steps=28,\n",
    "                    controlnet_conditioning_scale=0.7,\n",
    "                    guidance_scale=3.5,\n",
    "                    generator=generator,\n",
    "                ).images[0]\n",
    "            images.append(image)\n",
    "        image_logs.append({\"validation_image\": v_image, \"images\": images, \"validation_prompt\": v_prompt})\n",
    "\n",
    "    # Display validation results\n",
    "    for log in image_logs:\n",
    "        grid = make_image_grid([log[\"validation_image\"]] + log[\"images\"], 1, config.num_validation_images + 1)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.imshow(np.array(grid))\n",
    "        plt.title(f\"Step {step}: {log['validation_prompt']}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    flux_controlnet.train()\n",
    "    free_memory()\n",
    "    return image_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5702599b-266f-4e69-af13-766e1a23c35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/3750 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/50 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m noisy_latents \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m pixel_latents \u001b[38;5;241m+\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m noise\n\u001b[1;32m     51\u001b[0m guidance_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((bsz,), \u001b[38;5;241m3.5\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 53\u001b[0m controlnet_block_samples, controlnet_single_block_samples \u001b[38;5;241m=\u001b[39m \u001b[43mflux_controlnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet_added_conditions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooled_prompt_embeds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet_added_conditions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_image_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m flux_transformer(\n\u001b[1;32m     66\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mnoisy_latents,\n\u001b[1;32m     67\u001b[0m     timestep\u001b[38;5;241m=\u001b[39mtimesteps \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, (noise \u001b[38;5;241m-\u001b[39m pixel_latents), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/controlnets/controlnet_flux.py:354\u001b[0m, in \u001b[0;36mFluxControlNetModel.forward\u001b[0;34m(self, hidden_states, controlnet_cond, controlnet_mode, conditioning_scale, encoder_hidden_states, pooled_projections, timestep, img_ids, txt_ids, guidance, joint_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    345\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    346\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    351\u001b[0m         )\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         encoder_hidden_states, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     block_samples \u001b[38;5;241m=\u001b[39m block_samples \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    362\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([encoder_hidden_states, hidden_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/transformers/transformer_flux.py:173\u001b[0m, in \u001b[0;36mFluxTransformerBlock.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, temb, image_rotary_emb, joint_attention_kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    167\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mFloatTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     joint_attention_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    172\u001b[0m ):\n\u001b[0;32m--> 173\u001b[0m     norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1_context(\n\u001b[1;32m    176\u001b[0m         encoder_hidden_states, emb\u001b[38;5;241m=\u001b[39mtemb\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     joint_attention_kwargs \u001b[38;5;241m=\u001b[39m joint_attention_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/normalization.py:169\u001b[0m, in \u001b[0;36mAdaLayerNormZero.forward\u001b[0;34m(self, x, timestep, class_labels, hidden_dtype, emb)\u001b[0m\n\u001b[1;32m    167\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(timestep, class_labels, hidden_dtype\u001b[38;5;241m=\u001b[39mhidden_dtype)\n\u001b[1;32m    168\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msilu(emb))\n\u001b[0;32m--> 169\u001b[0m shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m6\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m scale_msa[:, \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m+\u001b[39m shift_msa[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, gate_msa, shift_mlp, scale_mlp, gate_mlp\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 2)"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "learning_rates = [optimizer.param_groups[0][\"lr\"]]\n",
    "losses = []\n",
    "mb = master_bar(range(config.max_train_steps // config.gradient_accumulation_steps))\n",
    "\n",
    "for epoch in mb:\n",
    "    pb = progress_bar(dataloader, parent=mb)\n",
    "    for i_batch, batch in enumerate(pb):\n",
    "        if global_step >= config.max_train_steps:\n",
    "            break\n",
    "\n",
    "        # Accumulate gradients\n",
    "        for _ in range(config.gradient_accumulation_steps):\n",
    "            with torch.no_grad():\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)\n",
    "                pixel_latents = FluxControlNetPipeline._pack_latents(\n",
    "                    vae.encode(pixel_values).latent_dist.sample(),\n",
    "                    config.batch_size, 16, config.resolution, config.resolution\n",
    "                )\n",
    "                control_image = FluxControlNetPipeline._pack_latents(\n",
    "                    vae.encode(batch[\"conditioning_pixel_values\"].to(device, dtype=torch.float32)).latent_dist.sample(),\n",
    "                    config.batch_size, 16, config.resolution, config.resolution\n",
    "                )\n",
    "                latent_image_ids = FluxControlNetPipeline._prepare_latent_image_ids(\n",
    "                    config.batch_size, config.resolution // 2, config.resolution // 2, device, torch.float32\n",
    "                )\n",
    "\n",
    "            noise = torch.randn_like(pixel_latents, device=device, dtype=torch.float32)\n",
    "            bsz = pixel_latents.shape[0]\n",
    "            timesteps = torch.rand(bsz, device=device) * noise_scheduler.config.num_train_timesteps\n",
    "            noisy_latents = (1 - timesteps.view(-1, 1, 1)) * pixel_latents + timesteps.view(-1, 1, 1) * noise\n",
    "            guidance_vec = torch.full((bsz,), 3.5, device=device, dtype=torch.float32)\n",
    "\n",
    "            controlnet_block_samples, controlnet_single_block_samples = flux_controlnet(\n",
    "                hidden_states=noisy_latents,\n",
    "                controlnet_cond=control_image,\n",
    "                timestep=timesteps / 1000,\n",
    "                guidance=guidance_vec,\n",
    "                pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device, dtype=torch.float32),\n",
    "                encoder_hidden_states=batch[\"prompt_ids\"].to(device, dtype=torch.float32),\n",
    "                txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(device, dtype=torch.float32),\n",
    "                img_ids=latent_image_ids,\n",
    "                return_dict=False,\n",
    "            )\n",
    "\n",
    "            noise_pred = flux_transformer(\n",
    "                hidden_states=noisy_latents,\n",
    "                timestep=timesteps / 1000,\n",
    "                guidance=guidance_vec,\n",
    "                pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device, dtype=torch.float32),\n",
    "                encoder_hidden_states=batch[\"prompt_ids\"].to(device, dtype=torch.float32),\n",
    "                controlnet_block_samples=controlnet_block_samples,\n",
    "                controlnet_single_block_samples=controlnet_single_block_samples,\n",
    "                txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(device, dtype=torch.float32),\n",
    "                img_ids=latent_image_ids,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            loss = F.mse_loss(noise_pred, (noise - pixel_latents), reduction=\"mean\")\n",
    "            loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        torch.nn.utils.clip_grad_norm_(flux_controlnet.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging\n",
    "        losses.append(loss.item())\n",
    "        learning_rates.append(optimizer.param_groups[0][\"lr\"])\n",
    "        pb.comment = f\"Loss: {losses[-1]:.4f}, LR: {learning_rates[-1]:.2e}\"\n",
    "        global_step += 1\n",
    "\n",
    "        # Checkpointing\n",
    "        if global_step % config.checkpointing_steps == 0:\n",
    "            save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "            flux_controlnet.save_pretrained(save_path)\n",
    "            print(f\"Saved checkpoint to {save_path}\")\n",
    "\n",
    "        # Validation\n",
    "        if global_step % config.validation_steps == 0:\n",
    "            image_logs = log_validation(global_step)\n",
    "\n",
    "    if global_step >= config.max_train_steps:\n",
    "        break\n",
    "\n",
    "# In[8]: Save Final Model\n",
    "flux_controlnet.save_pretrained(config.output_dir)\n",
    "final_image_logs = log_validation(global_step)\n",
    "\n",
    "if config.push_to_hub:\n",
    "    repo_id = create_repo(repo_id=config.hub_model_id, exist_ok=True).repo_id\n",
    "    readme = f\"\"\"\n",
    "# FLUX.1 ControlNet for Codec Enhancement\n",
    "Trained on {config.pretrained_model_name_or_path} to enhance outputs of a neural image codec.\n",
    "Dataset: danjacobellis/LSDIR_512_f16c12\n",
    "\"\"\"\n",
    "    for i, log in enumerate(final_image_logs):\n",
    "        grid = make_image_grid([log[\"validation_image\"]] + log[\"images\"], 1, config.num_validation_images + 1)\n",
    "        grid.save(os.path.join(config.output_dir, f\"example_{i}.png\"))\n",
    "        readme += f\"\\n![Example {i}](example_{i}.png)\\nPrompt: {log['validation_prompt']}\\n\"\n",
    "\n",
    "    with open(os.path.join(config.output_dir, \"README.md\"), \"w\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=config.output_dir,\n",
    "        commit_message=\"Final trained ControlNet\",\n",
    "        ignore_patterns=[\"checkpoint-*\"],\n",
    "    )\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
