{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7e2ba9-819e-44af-a038-d518e3ee690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device=\"cuda:0\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import datasets\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    FluxTransformer2DModel,\n",
    "    FluxControlNetModel,\n",
    "    FluxControlNetPipeline,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    CLIPTextModel,\n",
    "    T5EncoderModel,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from IPython.display import display\n",
    "from torchvision.transforms.v2 import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a750ecdb-e1ec-4250-bc93-cd00416b9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\",\n",
    "    dataset_name=\"danjacobellis/LSDIR_512_f16c12\",\n",
    "    image_column=\"image\",\n",
    "    conditioning_image_column=\"conditioning_image\",\n",
    "    caption_column=\"text\",\n",
    "    output_dir=\"controlnet_flux_output\",\n",
    "    resolution=512,\n",
    "    learning_rate=1e-5,\n",
    "    max_train_steps=15000,\n",
    "    validation_steps=100,\n",
    "    checkpointing_steps=200,\n",
    "    train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_double_layers=4,\n",
    "    num_single_layers=0,\n",
    "    seed=42,\n",
    "    num_validation_images=2,\n",
    "    guidance_scale=3.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ff158-43ad-4fa0-868b-828900a9f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "# ## Cell 3: Load Dataset and Add Dummy Captions\n",
    "\n",
    "dataset = datasets.load_dataset(config.dataset_name, split=\"train\")\n",
    "# Add a \"text\" column with empty strings since captions are not used\n",
    "dataset = dataset.map(lambda x: {\"text\": \"\"}, batched=True)\n",
    "print(f\"Dataset loaded with {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b9a237-712d-4470-826d-8db8f4bfe109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c2328acd3d4d45a4aaa0eee6375f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b494b5d158347b8be938a96bb51a84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b534b4ffcc94bf791422a0b675389bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=\"no\",  # Use float32\n",
    ")\n",
    "\n",
    "# Load tokenizers and text encoders\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"tokenizer_2\"\n",
    ")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "text_encoder_two = T5EncoderModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder_2\"\n",
    ")\n",
    "\n",
    "# Load VAE and Flux transformer\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "flux_transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714fb6a1-2fcd-4adc-ac0f-4abc13be8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_controlnet = FluxControlNetModel.from_transformer(\n",
    "    flux_transformer,\n",
    "    num_layers=config.num_double_layers,\n",
    "    num_single_layers=config.num_single_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0cfbfe-d061-4ee8-9108-13ed174687dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129fad75-3397-4b05-afe5-895c0acc0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FluxControlNetPipeline(\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder_one,\n",
    "    tokenizer=tokenizer_one,\n",
    "    text_encoder_2=text_encoder_two,\n",
    "    tokenizer_2=tokenizer_two,\n",
    "    transformer=flux_transformer,\n",
    "    controlnet=flux_controlnet,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2c8a597-0eef-4d63-8bbe-d2d998d061fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device, dtype=torch.float32).requires_grad_(False)\n",
    "flux_transformer.to(device, dtype=torch.float32).requires_grad_(False)\n",
    "text_encoder_one.to(device, dtype=torch.float32).requires_grad_(False)\n",
    "text_encoder_two.to(device, dtype=torch.float32).requires_grad_(False)\n",
    "flux_controlnet.to(device, dtype=torch.float32).train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16ddd82-ca47-4c35-a37d-c207f10f4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(config.resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # Scale to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e48b19-2302-4106-a795-4fc506896bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(examples):\n",
    "    images = [image_transforms(image.convert(\"RGB\")) for image in examples[config.image_column]]\n",
    "    conditioning_images = [image_transforms(image.convert(\"RGB\")) for image in examples[config.conditioning_image_column]]\n",
    "    return {\n",
    "        \"pixel_values\": images,\n",
    "        \"conditioning_pixel_values\": conditioning_images,\n",
    "    }\n",
    "\n",
    "# Compute text embeddings (all empty prompts)\n",
    "def compute_embeddings(batch, pipeline):\n",
    "    captions = batch[config.caption_column]  # List of empty strings\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(captions, prompt_2=captions)\n",
    "    text_ids = text_ids.unsqueeze(0).expand(len(captions), -1, -1)  # Expand to batch size\n",
    "    return {\n",
    "        \"prompt_embeds\": prompt_embeds.to(dtype=torch.float32),\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds.to(dtype=torch.float32),\n",
    "        \"text_ids\": text_ids.to(dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in examples])\n",
    "    prompt_embeds = torch.stack([example[\"prompt_embeds\"] for example in examples])\n",
    "    pooled_prompt_embeds = torch.stack([example[\"pooled_prompt_embeds\"] for example in examples])\n",
    "    text_ids = torch.stack([example[\"text_ids\"] for example in examples])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        \"prompt_ids\": prompt_embeds,\n",
    "        \"unet_added_conditions\": {\"pooled_prompt_embeds\": pooled_prompt_embeds, \"time_ids\": text_ids},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e764e-9619-4111-834f-4faeafd48736",
   "metadata": {},
   "outputs": [],
   "source": [
    "with accelerator.main_process_first():\n",
    "    train_dataset = dataset.with_transform(preprocess_batch)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda batch: compute_embeddings(batch, pipeline),\n",
    "        batched=True,\n",
    "        batch_size=50,\n",
    "    )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=config.train_batch_size,\n",
    "    num_workers=0,  # Adjust based on your system\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b5be898-16e6-4805-ae62-da5425e8e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    flux_controlnet.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0, total_iters=config.max_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e7943-71c6-4c68-8814-8357894b5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    flux_controlnet, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931948b-3ae8-4de3-b318-80b542b58df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples = dataset.select(range(config.num_validation_images))\n",
    "validation_images = [sample[config.conditioning_image_column] for sample in validation_samples]\n",
    "validation_prompts = [\"\"] * len(validation_images)\n",
    "\n",
    "def log_validation(step):\n",
    "    pipeline.controlnet = accelerator.unwrap_model(flux_controlnet)\n",
    "    pipeline.to(device)\n",
    "    images = []\n",
    "    to_pil = ToPILImage()\n",
    "    for val_image, val_prompt in zip(validation_images, validation_prompts):\n",
    "        val_image = image_transforms(val_image.convert(\"RGB\")).to(device).unsqueeze(0)\n",
    "        prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt([val_prompt], prompt_2=[val_prompt])\n",
    "        with torch.no_grad():\n",
    "            gen_image = pipeline(\n",
    "                prompt_embeds=prompt_embeds,\n",
    "                pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                control_image=val_image,\n",
    "                num_inference_steps=28,\n",
    "                controlnet_conditioning_scale=0.7,\n",
    "                guidance_scale=config.guidance_scale,\n",
    "            ).images[0]\n",
    "        images.append(gen_image)\n",
    "    print(f\"Validation at step {step}:\")\n",
    "    for orig, cond, gen in zip(\n",
    "        [sample[config.image_column] for sample in validation_samples],\n",
    "        validation_images,\n",
    "        images\n",
    "    ):\n",
    "        display(to_pil(image_transforms(orig.convert(\"RGB\")).clamp(-1, 1) / 2 + 0.5))  # Original\n",
    "        display(to_pil(val_image[0].clamp(-1, 1) / 2 + 0.5))  # Condition\n",
    "        display(gen)  # Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723591f-c35e-4f85-a93b-cb12e578c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = config.train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps\n",
    "print(f\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Total optimization steps = {config.max_train_steps}\")\n",
    "print(f\"  Total train batch size = {total_batch_size}\")\n",
    "\n",
    "progress_bar = tqdm(range(config.max_train_steps), desc=\"Steps\")\n",
    "global_step = 0\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    with accelerator.accumulate(flux_controlnet):\n",
    "        # Encode images to latents\n",
    "        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)\n",
    "        pixel_latents_tmp = vae.encode(pixel_values).latent_dist.sample()\n",
    "        pixel_latents = pipeline._pack_latents(\n",
    "            (pixel_latents_tmp - vae.config.shift_factor) * vae.config.scaling_factor,\n",
    "            pixel_values.shape[0],\n",
    "            pixel_latents_tmp.shape[1],\n",
    "            pixel_latents_tmp.shape[2],\n",
    "            pixel_latents_tmp.shape[3],\n",
    "        )\n",
    "\n",
    "        control_values = batch[\"conditioning_pixel_values\"].to(device, dtype=torch.float32)\n",
    "        control_latents = vae.encode(control_values).latent_dist.sample()\n",
    "        control_image = pipeline._pack_latents(\n",
    "            (control_latents - vae.config.shift_factor) * vae.config.scaling_factor,\n",
    "            control_values.shape[0],\n",
    "            control_latents.shape[1],\n",
    "            control_latents.shape[2],\n",
    "            control_latents.shape[3],\n",
    "        )\n",
    "\n",
    "        latent_image_ids = pipeline._prepare_latent_image_ids(\n",
    "            batch_size=pixel_latents_tmp.shape[0],\n",
    "            height=pixel_latents_tmp.shape[2] // 2,\n",
    "            width=pixel_latents_tmp.shape[3] // 2,\n",
    "            device=device,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        # Sample noise and timesteps\n",
    "        noise = torch.randn_like(pixel_latents, device=device, dtype=torch.float32)\n",
    "        bsz = pixel_latents.shape[0]\n",
    "        timesteps = torch.sigmoid(torch.randn((bsz,), device=device, dtype=torch.float32))\n",
    "\n",
    "        # Flow matching\n",
    "        noisy_latents = (1 - timesteps.view(-1, 1, 1)) * pixel_latents + timesteps.view(-1, 1, 1) * noise\n",
    "        guidance_vec = torch.full((bsz,), config.guidance_scale, device=device, dtype=torch.float32)\n",
    "\n",
    "        # ControlNet forward pass\n",
    "        controlnet_block_samples, controlnet_single_block_samples = flux_controlnet(\n",
    "            hidden_states=noisy_latents,\n",
    "            controlnet_cond=control_image,\n",
    "            timestep=timesteps,\n",
    "            guidance=guidance_vec,\n",
    "            pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device, dtype=torch.float32),\n",
    "            encoder_hidden_states=batch[\"prompt_ids\"].to(device, dtype=torch.float32),\n",
    "            txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(device, dtype=torch.float32),\n",
    "            img_ids=latent_image_ids,\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "        # Transformer forward pass with ControlNet outputs\n",
    "        noise_pred = flux_transformer(\n",
    "            hidden_states=noisy_latents,\n",
    "            timestep=timesteps,\n",
    "            guidance=guidance_vec,\n",
    "            pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(device, dtype=torch.float32),\n",
    "            encoder_hidden_states=batch[\"prompt_ids\"].to(device, dtype=torch.float32),\n",
    "            controlnet_block_samples=[sample.to(dtype=torch.float32) for sample in controlnet_block_samples],\n",
    "            controlnet_single_block_samples=[sample.to(dtype=torch.float32) for sample in controlnet_single_block_samples],\n",
    "            txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(device, dtype=torch.float32),\n",
    "            img_ids=latent_image_ids,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(noise_pred, (noise - pixel_latents), reduction=\"mean\")\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if accelerator.sync_gradients:\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "        progress_bar.set_postfix({\"loss\": loss.item(), \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "        # Checkpointing\n",
    "        if global_step % config.checkpointing_steps == 0:\n",
    "            save_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "            accelerator.save_state(save_path)\n",
    "            print(f\"Saved checkpoint to {save_path}\")\n",
    "\n",
    "        # Validation\n",
    "        if global_step % config.validation_steps == 0:\n",
    "            log_validation(global_step)\n",
    "\n",
    "    if global_step >= config.max_train_steps:\n",
    "        break\n",
    "\n",
    "# ## Cell 10: Save Final Model\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    flux_controlnet = accelerator.unwrap_model(flux_controlnet)\n",
    "    flux_controlnet.save_pretrained(config.output_dir)\n",
    "    print(f\"Final model saved to {config.output_dir}\")\n",
    "\n",
    "# Perform final validation\n",
    "log_validation(\"final\")\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
