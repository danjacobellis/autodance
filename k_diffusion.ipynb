{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a38491-f887-4b7c-953c-09520684b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-cbafb023-40ef-594e-9092-fb0e3c44baa2\n",
    "device=\"cuda:0\"\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.v2.functional import pil_to_tensor, to_pil_image\n",
    "from types import SimpleNamespace\n",
    "from k_diffusion.models.image_transformer_v2 import (\n",
    "    ImageTransformerDenoiserModelV2,\n",
    "    GlobalAttentionSpec,\n",
    "    NeighborhoodAttentionSpec\n",
    ")\n",
    "from collections import namedtuple\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from timm.optim import Mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a971c1-4b2d-4df1-9016-c4fb3595b73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a26e00bd87430dbe566db65d570e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08bbff1458e42538629f33e1335e88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cfa6ff05e24908a966f81e269309b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_512_f16c12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e685fb62-cedf-4247-bf4e-34ab5fc0157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.lr_pow = 2\n",
    "config.batch_size = 2\n",
    "config.max_lr = (64/config.batch_size)*1e-3\n",
    "config.min_lr = config.max_lr / 1e3\n",
    "config.num_workers = 32\n",
    "config.epochs = 36\n",
    "config.total_steps = config.epochs * (dataset['train'].num_rows // config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc80f77-5172-4f00-aa16-23f9831596e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x1 = torch.cat([\n",
    "        pil_to_tensor(sample['image']).unsqueeze(0)\n",
    "        for sample in batch\n",
    "    ]).to(torch.float)/127.5 - 1.0\n",
    "    x2 = torch.cat([\n",
    "        pil_to_tensor(sample['conditioning_image']).unsqueeze(0)\n",
    "        for sample in batch\n",
    "    ]).to(torch.float)/127.5 - 1.0\n",
    "    return x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bcb130-b8ba-4a47-984b-1b5703bec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "LevelSpec = namedtuple('LevelSpec', ['width', 'depth', 'd_ff', 'self_attn', 'dropout'])\n",
    "MappingSpec = namedtuple('MappingSpec', ['depth', 'width', 'd_ff', 'dropout'])\n",
    "\n",
    "levels = [\n",
    "    LevelSpec(width=128, depth=2, d_ff=512, self_attn=NeighborhoodAttentionSpec(d_head=64, kernel_size=7), dropout=0.1),\n",
    "    LevelSpec(width=256, depth=2, d_ff=1024, self_attn=NeighborhoodAttentionSpec(d_head=64, kernel_size=7), dropout=0.1),\n",
    "    LevelSpec(width=512, depth=2, d_ff=2048, self_attn=NeighborhoodAttentionSpec(d_head=64, kernel_size=7), dropout=0.1),\n",
    "    LevelSpec(width=1024, depth=4, d_ff=4096, self_attn=GlobalAttentionSpec(d_head=64), dropout=0.1)\n",
    "]\n",
    "mapping = MappingSpec(depth=4, width=512, d_ff=2048, dropout=0.1)\n",
    "\n",
    "# Initialize the model\n",
    "model = ImageTransformerDenoiserModelV2(\n",
    "    levels=levels,\n",
    "    mapping=mapping,\n",
    "    in_channels=6,         # 3 channels for noisy image + 3 for lossy reconstruction\n",
    "    out_channels=3,        # Output is the predicted clean RGB image\n",
    "    patch_size=(4,4),      # Patch size for tokenization\n",
    "    num_classes=0,         # No class conditioning\n",
    "    mapping_cond_dim=0     # No additional mapping conditioning\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ed4fc3-fea9-42eb-b315-8c95c0bd875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_sigma(t, eta=0.5, log_snr_min=-15, log_snr_max=15):\n",
    "    log_snr = -2 * (torch.log(torch.tan(math.pi * t / 2)) + math.log(eta))\n",
    "    log_snr = torch.clamp(log_snr, log_snr_min, log_snr_max)\n",
    "    alpha_sq = torch.sigmoid(log_snr)\n",
    "    sigma_sq = torch.sigmoid(-log_snr)\n",
    "    return torch.sqrt(alpha_sq), torch.sqrt(sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f185d11-937f-4956-9ba0-a15064063935",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Mars(model.parameters(), lr=1.0, caution=True)\n",
    "\n",
    "def rc_sched(i_step, config):\n",
    "    t = i_step / config.total_steps\n",
    "    return (config.max_lr - config.min_lr) * (1 - ((np.cos(np.pi*t))**(2*config.lr_pow))) + config.min_lr\n",
    "\n",
    "schedule = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda i_step: rc_sched(i_step, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bac9e-b68d-4b7b-910d-f5cf6f31b187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/36 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='42495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/42495 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning_rates = [optimizer.param_groups[0]['lr']]\n",
    "mb = master_bar(range(config.epochs))\n",
    "losses = []\n",
    "\n",
    "global_step = 0\n",
    "model.train()\n",
    "for i_epoch in mb:\n",
    "    model.train()\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "            dataset['train'],\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            collate_fn = collate_fn\n",
    "        )\n",
    "    pb = progress_bar(dataloader_train, parent=mb)\n",
    "    for i_batch, (x, x_lossy) in enumerate(pb):\n",
    "        x = x.to(device)\n",
    "        x_lossy = x_lossy.to(device)\n",
    "        batch_size = x.shape[0]\n",
    "        t = torch.rand(batch_size, device=device) * (1.0 - 1e-3) + 1e-3\n",
    "        alpha_t, sigma_t = get_alpha_sigma(t)\n",
    "        epsilon = torch.randn_like(x)\n",
    "        x_noisy = alpha_t.view(-1, 1, 1, 1) * x + sigma_t.view(-1, 1, 1, 1) * epsilon\n",
    "        x_input = torch.cat([x_noisy, x_lossy], dim=1)  # [batch, 6, 512, 512]\n",
    "        x_pred = model(x_input, sigma_t)  # [batch, 3, 512, 512]\n",
    "        loss = F.mse_loss(x_pred, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
