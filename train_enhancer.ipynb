{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277a7639-27fb-4538-8ef3-be369d5347d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device=\"cuda:0\"\n",
    "import torch\n",
    "import datasets\n",
    "import io\n",
    "from types import SimpleNamespace\n",
    "from codec import AutoEncoderND\n",
    "from torchvision.transforms.v2 import Compose, CenterCrop, ToTensor, Normalize, ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6cef80-95c4-49aa-ae15-9dec28570923",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.resolution = 512\n",
    "config.codec_checkpoint = \"../hf/dance/LF_rgb_f16c12_v1.0.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c34430-5a31-4299-8ff6-92712050ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(config.codec_checkpoint, map_location=device, weights_only=False)\n",
    "codec_config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "codec = AutoEncoderND(\n",
    "    dim=2,\n",
    "    input_channels=codec_config.input_channels,\n",
    "    J=int(codec_config.F**0.5),\n",
    "    latent_dim=codec_config.latent_dim,\n",
    "    lightweight_encode=codec_config.lightweight_encode,\n",
    "    lightweight_decode=codec_config.lightweight_decode\n",
    ").to(device)\n",
    "codec.load_state_dict(state_dict)\n",
    "codec.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbcad2-7b0b-4562-b4fe-290836c7fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1bae0f5e504f78b7fa430c890986db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f335b88f334a4d8355188b323f2a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75c041e2ee745df8204e42e5361f94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f283daa42e2b43f19f76cfb147b518ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_540\",split='train')\n",
    "\n",
    "transform = Compose([\n",
    "    CenterCrop(config.resolution),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def gen_conditioning_img(sample):\n",
    "    x = transform(sample['image']).to(device).unsqueeze(0)\n",
    "    img = ToPILImage()(x[0]/2 + 0.5)\n",
    "    buff = io.BytesIO()\n",
    "    img.save(buff, format='WEBP', lossless=True)\n",
    "    webp_bytes = buff.getbuffer()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xhat = codec.decode(codec.quantize.compand(codec.encode(x)).round()).clamp(-1,1)\n",
    "\n",
    "    img2 = ToPILImage()(xhat[0]/2 + 0.5)\n",
    "    buff2 = io.BytesIO()\n",
    "    img2.save(buff2, format='WEBP', lossless=True)\n",
    "    webp_bytes2 = buff2.getbuffer()\n",
    "    return {\n",
    "        'image': webp_bytes,\n",
    "        'conditioning_image': webp_bytes2\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(gen_conditioning_img,writer_batch_size=500).cast_column('image',datasets.Image()).cast_column('conditioning_image',datasets.Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7450f-a746-4a0f-b68d-e984e6f2a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.push_to_hub(\"danjacobellis/LSDIR_512_f16c12\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15af6f-2fe9-4c87-802d-276e0d89a9a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eee190-8933-4695-b6b3-db1d73c82196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/diffusers/refs/heads/main/examples/controlnet/train_controlnet_flux.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30529c48-04bd-4f51-a2a1-ef4a8b1fcb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device = \"cuda:0\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import datasets\n",
    "from types import SimpleNamespace\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from PIL import Image\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082648db-18e7-41da-b489-3b2f81ec327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "config = SimpleNamespace()\n",
    "config.pretrained_model_name_or_path = \"black-forest-labs/FLUX.1-dev\"  # Base FLUX model\n",
    "config.dataset_name = \"danjacobellis/LSDIR_512_f16c12\"  # Pre-computed dataset\n",
    "config.output_dir = \"controlnet_enhancer\"  # Where to save the model\n",
    "config.resolution = 160  # Matches dataset resolution\n",
    "config.train_batch_size = 4  # Recommended for 80GB A100\n",
    "config.gradient_accumulation_steps = 1  # Adjusted for batch size 2\n",
    "config.learning_rate = 1e-5  # From tutorial\n",
    "config.max_train_steps = 21250  # Total training steps\n",
    "config.validation_steps = 100  # Validate every 100 steps\n",
    "config.checkpointing_steps = 200  # Save checkpoint every 200 steps\n",
    "config.num_double_layers = 4  # ControlNet architecture\n",
    "config.num_single_layers = 0  # ControlNet architecture\n",
    "config.seed = 42  # For reproducibility\n",
    "config.report_to = \"tensorboard\"  # Logging\n",
    "config.mixed_precision = \"bf16\"  # Use float32 as requested\n",
    "config.allow_tf32 = True  # Speed up on A100\n",
    "config.max_train_samples = None  # Use full dataset; set to 1000 for testing\n",
    "config.checkpoints_total_limit = 3  # Keep only 3 checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ced080-99d4-4000-9de8-72b5f2d67750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "# fixed_prompt = \"a high-quality photograph\"  # Generic prompt for enhancement\n",
    "# dataset = dataset.map(lambda x: {\"text\": fixed_prompt})\n",
    "# dataset.push_to_hub(\"danjacobellis/LSDIR_512_f16c12_caption\",split='train')\n",
    "# val_samples = dataset.select([0, 1])  # Take first two samples for validation\n",
    "# os.makedirs(\"validation_images\", exist_ok=True)\n",
    "# for i, sample in enumerate(val_samples):\n",
    "#     cond_img = sample[\"conditioning_image\"]  # Codec-decoded image\n",
    "#     cond_img.save(f\"validation_images/cond_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5475ce-e8bb-4be1-ad80-1e8960080af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training command:\n",
      "\n",
      "CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8 accelerate launch train_controlnet_flux.py     --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\"     --dataset_name=\"danjacobellis/LSDIR_512_f16c12_caption\"     --conditioning_image_column=\"conditioning_image\"     --image_column=\"image\"     --caption_column=\"text\"     --output_dir=\"controlnet_enhancer\"     --mixed_precision=\"bf16\"     --resolution=160     --learning_rate=1e-05     --max_train_steps=21250     --validation_steps=100     --checkpointing_steps=200     --train_batch_size=4     --gradient_accumulation_steps=1     --report_to=\"tensorboard\"     --num_double_layers=4     --num_single_layers=0     --seed=42     --allow_tf32     --checkpoints_total_limit=3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Training Command\n",
    "training_command = f\"\"\"\n",
    "CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8 accelerate launch train_controlnet_flux.py \\\n",
    "    --pretrained_model_name_or_path=\"{config.pretrained_model_name_or_path}\" \\\n",
    "    --dataset_name=\"danjacobellis/LSDIR_512_f16c12_caption\" \\\n",
    "    --conditioning_image_column=\"conditioning_image\" \\\n",
    "    --image_column=\"image\" \\\n",
    "    --caption_column=\"text\" \\\n",
    "    --output_dir=\"{config.output_dir}\" \\\n",
    "    --mixed_precision=\"{config.mixed_precision}\" \\\n",
    "    --resolution={config.resolution} \\\n",
    "    --learning_rate={config.learning_rate} \\\n",
    "    --max_train_steps={config.max_train_steps} \\\n",
    "    --validation_steps={config.validation_steps} \\\n",
    "    --checkpointing_steps={config.checkpointing_steps} \\\n",
    "    --train_batch_size={config.train_batch_size} \\\n",
    "    --gradient_accumulation_steps={config.gradient_accumulation_steps} \\\n",
    "    --report_to=\"{config.report_to}\" \\\n",
    "    --num_double_layers={config.num_double_layers} \\\n",
    "    --num_single_layers={config.num_single_layers} \\\n",
    "    --seed={config.seed} \\\n",
    "    --allow_tf32 \\\n",
    "    --checkpoints_total_limit={config.checkpoints_total_limit}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the training command\n",
    "print(\"Running training command:\")\n",
    "print(training_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31ad81-195e-43af-acd7-21871fb3e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 6: Inference Example\n",
    "# import torch\n",
    "# from diffusers.utils import load_image\n",
    "# from diffusers.pipelines.flux.pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "# from diffusers.models.controlnet_flux import FluxControlNetModel\n",
    "\n",
    "# # Load the trained model\n",
    "# controlnet = FluxControlNetModel.from_pretrained(config.output_dir, torch_dtype=torch.float32)\n",
    "# pipe = FluxControlNetPipeline.from_pretrained(\n",
    "#     config.pretrained_model_name_or_path,\n",
    "#     controlnet=controlnet,\n",
    "#     torch_dtype=torch.float32\n",
    "# )\n",
    "# pipe.to(device)\n",
    "\n",
    "# # Load a test conditioning image\n",
    "# control_image = load_image(\"validation_images/cond_0.png\").resize((512, 512))\n",
    "# prompt = \"a high-quality photograph\"\n",
    "\n",
    "# # Generate enhanced image\n",
    "# image = pipe(\n",
    "#     prompt,\n",
    "#     control_image=control_image,\n",
    "#     controlnet_conditioning_scale=0.7,\n",
    "#     num_inference_steps=28,\n",
    "#     guidance_scale=3.5,\n",
    "# ).images[0]\n",
    "# image.save(\"enhanced_output.png\")\n",
    "# display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
