{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277a7639-27fb-4538-8ef3-be369d5347d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-768d9c1d-110f-52e2-b0a2-3252f78280f8\n",
    "device=\"cuda:0\"\n",
    "import torch\n",
    "import datasets\n",
    "import io\n",
    "from types import SimpleNamespace\n",
    "from codec import AutoEncoderND\n",
    "from torchvision.transforms.v2 import Compose, CenterCrop, ToTensor, Normalize, ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6cef80-95c4-49aa-ae15-9dec28570923",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.resolution = 512\n",
    "config.codec_checkpoint = \"../hf/dance/LF_rgb_f16c12_v1.0.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c34430-5a31-4299-8ff6-92712050ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(config.codec_checkpoint, map_location=device, weights_only=False)\n",
    "codec_config = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "codec = AutoEncoderND(\n",
    "    dim=2,\n",
    "    input_channels=codec_config.input_channels,\n",
    "    J=int(codec_config.F**0.5),\n",
    "    latent_dim=codec_config.latent_dim,\n",
    "    lightweight_encode=codec_config.lightweight_encode,\n",
    "    lightweight_decode=codec_config.lightweight_decode\n",
    ").to(device)\n",
    "codec.load_state_dict(state_dict)\n",
    "codec.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbcad2-7b0b-4562-b4fe-290836c7fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1bae0f5e504f78b7fa430c890986db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f335b88f334a4d8355188b323f2a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75c041e2ee745df8204e42e5361f94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgj335/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f283daa42e2b43f19f76cfb147b518ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"danjacobellis/LSDIR_540\",split='train')\n",
    "\n",
    "transform = Compose([\n",
    "    CenterCrop(config.resolution),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def gen_conditioning_img(sample):\n",
    "    x = transform(sample['image']).to(device).unsqueeze(0)\n",
    "    img = ToPILImage()(x[0]/2 + 0.5)\n",
    "    buff = io.BytesIO()\n",
    "    img.save(buff, format='WEBP', lossless=True)\n",
    "    webp_bytes = buff.getbuffer()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xhat = codec.decode(codec.quantize.compand(codec.encode(x)).round()).clamp(-1,1)\n",
    "\n",
    "    img2 = ToPILImage()(xhat[0]/2 + 0.5)\n",
    "    buff2 = io.BytesIO()\n",
    "    img2.save(buff2, format='WEBP', lossless=True)\n",
    "    webp_bytes2 = buff2.getbuffer()\n",
    "    return {\n",
    "        'image': webp_bytes,\n",
    "        'conditioning_image': webp_bytes2\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(gen_conditioning_img,writer_batch_size=500).cast_column('image',datasets.Image()).cast_column('conditioning_image',datasets.Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7450f-a746-4a0f-b68d-e984e6f2a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.push_to_hub(\"danjacobellis/LSDIR_512_f16c12\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e332b66-823d-488a-a3db-c7c5543afac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2025 20:32:03 - INFO - __main__ - Pre-processing dataset to generate conditioning images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9377ae0184723bce16ceef4672547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Pre-processing dataset to generate conditioning images...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f07991-9d5b-48c4-b3da-34fbbdb12e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(f\"Uploading pre-processed dataset to {config.preprocessed_dataset_name}\")\n",
    "# new_dataset.push_to_hub(config.preprocessed_dataset_name, split='train')\n",
    "# logger.info(\"Dataset pre-processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3836ae9-aa51-4d09-a6e7-c40471b22368",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProjectConfiguration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m Accelerator(\n\u001b[1;32m      2\u001b[0m     mixed_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Use float32 as specified\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     log_with\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     project_config\u001b[38;5;241m=\u001b[39m\u001b[43mProjectConfiguration\u001b[49m(project_dir\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_dir, logging_dir\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlogging_dir)\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(accelerator\u001b[38;5;241m.\u001b[39mstate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ProjectConfiguration' is not defined"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"no\",  # Use float32 as specified\n",
    "    log_with=\"tensorboard\",\n",
    "    project_config=ProjectConfiguration(project_dir=config.output_dir, logging_dir=config.logging_dir)\n",
    ")\n",
    "logger.info(accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4afb44-b498-4ad7-9602-00dc74eb8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load Flux models and tokenizers\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ").to(device, dtype=torch.float32)\n",
    "flux_transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"transformer\"\n",
    ").to(device, dtype=torch.float32)\n",
    "flux_controlnet = FluxControlNetModel.from_transformer(\n",
    "    flux_transformer,\n",
    "    num_layers=config.num_double_layers,\n",
    "    num_single_layers=config.num_single_layers\n",
    ").to(device, dtype=torch.float32)\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(config.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(config.pretrained_model_name_or_path, subfolder=\"tokenizer_2\")\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ").to(device, dtype=torch.float32)\n",
    "text_encoder_two = T5EncoderModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder_2\"\n",
    ").to(device, dtype=torch.float32)\n",
    "\n",
    "# Freeze non-ControlNet models\n",
    "vae.requires_grad_(False)\n",
    "flux_transformer.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "flux_controlnet.train()\n",
    "\n",
    "# Define the training command (to be run via accelerate)\n",
    "training_command = (\n",
    "    f\"accelerate launch train_controlnet_flux.py \"\n",
    "    f\"--pretrained_model_name_or_path={config.pretrained_model_name_or_path} \"\n",
    "    f\"--dataset_name={config.preprocessed_dataset_name} \"\n",
    "    f\"--image_column=image \"\n",
    "    f\"--conditioning_image_column=conditioning_image \"\n",
    "    f\"--caption_column=text \"\n",
    "    f\"--resolution={config.resolution} \"\n",
    "    f\"--train_batch_size={config.train_batch_size} \"\n",
    "    f\"--mixed_precision=no \"  # Float32 precision\n",
    "    f\"--num_double_layers={config.num_double_layers} \"\n",
    "    f\"--num_single_layers={config.num_single_layers} \"\n",
    "    f\"--max_train_steps={config.max_train_steps} \"\n",
    "    f\"--checkpointing_steps={config.checkpointing_steps} \"\n",
    "    f\"--validation_steps={config.validation_steps} \"\n",
    "    f\"--learning_rate={config.learning_rate} \"\n",
    "    f\"--output_dir={config.output_dir} \"\n",
    "    f\"--logging_dir={config.logging_dir} \"\n",
    "    f\"--push_to_hub \"\n",
    "    f\"--hub_model_id={config.hub_model_id}\"\n",
    ")\n",
    "\n",
    "logger.info(\"Training command prepared:\")\n",
    "logger.info(training_command)\n",
    "\n",
    "# Note: Save this command to a script or run it in a terminal after setting up accelerate config\n",
    "# For notebook execution, you might need to use ! or os.system() if running directly:\n",
    "# ! {training_command}\n",
    "# Alternatively, save and run separately after `accelerate config`\n",
    "\n",
    "# ## Cell 6: Post-Training Notes\n",
    "\n",
    "# After training, the ControlNet model will be saved to `config.output_dir` and pushed to the Hugging Face Hub\n",
    "# at `config.hub_model_id`. You can use it for inference with the FluxControlNetPipeline as shown in the tutorial.\n",
    "\n",
    "# Example inference (after training):\n",
    "\"\"\"\n",
    "from diffusers.pipelines.flux.pipeline_flux_controlnet import FluxControlNetPipeline\n",
    "pipe = FluxControlNetPipeline.from_pretrained(\n",
    "    config.pretrained_model_name_or_path,\n",
    "    controlnet=FluxControlNetModel.from_pretrained(config.output_dir),\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "# Add your inference code here with a conditioning image from the codec\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
