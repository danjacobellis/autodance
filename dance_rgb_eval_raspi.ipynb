{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e1e44-a706-49f7-b752-377bc99f00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/danjacobellis/dance/resolve/main/LF_rgb_f16c12_v1.6.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb7d1ed-cfe7-4a60-a5ef-4e7083205b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import io\n",
    "import numpy as np\n",
    "import datasets\n",
    "import time\n",
    "from types import SimpleNamespace\n",
    "from datasets import Dataset\n",
    "from torchvision.transforms.v2 import CenterCrop\n",
    "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b06d2d79-fd36-419d-bfc8-96579af4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_and_evaluate(sample, quality=0.1):\n",
    "    img = sample['image']\n",
    "    x_orig = pil_to_tensor(img).to(device).unsqueeze(0).to(torch.float) / 127.5 - 1.0\n",
    "    orig_dim = x_orig.numel() \n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x_orig)\n",
    "        latent = model.quantize.compand(z).round()\n",
    "    webp = latent_to_pil(latent.cpu(), n_bits=8, C=3)\n",
    "    buff = io.BytesIO()\n",
    "    webp[0].save(buff, format='WEBP', lossless=True)\n",
    "    neural_encode_time = time.time() - t0\n",
    "    \n",
    "    return {\n",
    "        'neural_encode_time': neural_encode_time,\n",
    "        'mp':orig_dim/3e6\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5324e50-c936-4374-a382-3029cb398692",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "valid_dataset = datasets.load_dataset(\"danjacobellis/LSDIR_val\", split='validation').select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96f2bd3e-e30c-4873-bf2d-1cbcb2ca23f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.868352\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('LF_rgb_f16c12_v1.6.pth', map_location=device,weights_only=False)\n",
    "config = checkpoint['config']\n",
    "# state_dict = checkpoint['state_dict']\n",
    "\n",
    "model = AutoCodecND(\n",
    "    dim=2,\n",
    "    input_channels=config.input_channels,\n",
    "    J=4,\n",
    "    latent_dim=12,\n",
    "    encoder_depth = 6,\n",
    "    encoder_kernel_size=3,\n",
    "    decoder_depth = 1,\n",
    "    lightweight_encode=config.lightweight_encode,\n",
    "    lightweight_decode=config.lightweight_decode,\n",
    ").to(device)\n",
    "# model.load_state_dict(state_dict)\n",
    "model.eval();\n",
    "print(sum(p.numel() for p in model.encoder_blocks.parameters())/1e6)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d30e515-2200-47cd-966c-f246ff51679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ks=1, d=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4837, 0.6055, 0.5899, 0.5406, 0.5649, 0.5846, 0.5747, 0.6035, 0.6152,\n",
       "        0.6324])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ks=1, d=1')\n",
    "r = valid_dataset.map(lambda batch: compress_and_evaluate(batch,0.45)).with_format('torch')\n",
    "r['mp'] / r['neural_encode_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0387f26f-5d18-43db-85ef-3f87338714b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ks=3, d=6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdfa18a9e16468899eb67f0d5c44765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1688, 0.1711, 0.1713, 0.1725, 0.1811, 0.1735, 0.1797, 0.1820, 0.1745,\n",
       "        0.1773])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ks=3, d=6')\n",
    "r = valid_dataset.map(lambda batch: compress_and_evaluate(batch,0.45)).with_format('torch')\n",
    "r['mp'] / r['neural_encode_time']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
