{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb7d1ed-cfe7-4a60-a5ef-4e7083205b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import io\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torchaudio\n",
    "import einops\n",
    "import tempfile\n",
    "from IPython.display import Audio as play\n",
    "from torchvision.transforms.v2 import CenterCrop, PILToTensor, ToPILImage, Pad, CenterCrop\n",
    "from types import SimpleNamespace\n",
    "from autocodec.codec import AutoCodecND, latent_to_pil, pil_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5324e50-c936-4374-a382-3029cb398692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5d110b1e8649d99a0023defabd2550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64107679750e46c18c4aaccde476cb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d3f004e1d3451dba7f586c864f1b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c75e69569c423b8643b751e95faa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1830e75b6cde4ebfb6c2e598ef47e93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "valid_dataset = dataset = datasets.load_dataset(\"danjacobellis/musdb_segments\",split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ed372f-663c-40c1-87dd-dbeb881b30eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('../hf/dance/LF_stereo_f512c16_v1.0.pth', map_location=device,weights_only=False)\n",
    "config = checkpoint['config']\n",
    "model = AutoCodecND(\n",
    "    dim=1,\n",
    "    input_channels=config.input_channels,\n",
    "    J=int(np.log2(config.F)),\n",
    "    latent_dim=config.latent_dim,\n",
    "    lightweight_encode=config.lightweight_encode,\n",
    "    lightweight_decode=config.lightweight_decode,\n",
    "    post_filter=config.post_filter\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c4f7f2-7c41-4f7c-b532-e048abfb1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2**19\n",
    "C = config.input_channels\n",
    "center_crop = CenterCrop((1, L))\n",
    "def valid_collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    x = torch.zeros((B, C, L), dtype=torch.float)\n",
    "    for i_sample, sample in enumerate(batch):\n",
    "        xi = torch.zeros((C, 1, 2**21), dtype=torch.int16)\n",
    "        audio_mix, fs = torchaudio.load(sample['audio_mix']['bytes'], normalize=False)\n",
    "        xi[:, 0, :] = audio_mix\n",
    "        xi = center_crop(xi).to(torch.float)\n",
    "        xi = xi.squeeze(1)\n",
    "        xi = xi - xi.mean()\n",
    "        max_abs = xi.abs().max()\n",
    "        xi = xi / (max_abs + 1e-8)\n",
    "        x[i_sample, :, :] = xi\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805ba5ae-c251-42f6-b3ca-f22ae2593606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/7gAOQWRvYmUAZAAAAAAA/9sAQwAIBgYHBgUIBwcHCQkICgwUDQwLCwwZEhMPFB0aHx4dGhwcICQuJyAiLCMcHCg3KSwwMTQ0NB8nOT04MjwuMzQy/8AAFAgAQABABEMRAE0RAFkRAEsRAP/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/aAA4EQwBNAFkASwAAPwBtNGO1Nzg9qb0pPyoo/H8qKOvFHfFH50cc8UtHbNHHpRSnr0PtS0HjrR0pPw/z/Sk6/wA6Qfyo/Kg9CKKX8qDmlNFH40UlFFFFFL9KKKPXNJ/Kij3o549aKMc0tFFFJRmij+Z70Cij3oo/Gj9KP8miijt0ozijIoxRzRR2oxSUUUdKM0dPSjt70vvxS0UfhQRRRR/k0fXiijHGaP1pPyFH4/lRjFB4opOozRR+FHTvRg+tGe9BA5oGaXnGetHTntRzmjNFGeetL9T3o4z/AJ/z/wDrpcYP+f8APpR+v+f8/nSUZwT/AJ/z/wDrpMY6UZ9qMUvtR07/AOf84o4H/wBejHoaM0Un50n4038KOBS0tL27UdqKKTv1o/Gj8aKPzo4ooz7UvHajtRRnvS/hRz60c0UlHSjjvmigGk49KM+1GKSl5ooooooo9KKKKKO9FFLRRR3oooooNFFH8qWiko/yabijpS0Zo6n3ooozSijtRRmkooopc9KTnrmj8eaO9GaOtFFHXpz6UdTRRg9aOT2NLR2oxSUUUUtFFFFFGKOKMUZopKM5FGTmjjpS5z2o69aM+1HtRRR6cUUZAoopR15xRn61/9k=",
      "text/plain": [
       "<PIL.Image.Image image mode=CMYK size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.33286285579864 compression ratio\n",
      "64.0× dimension reduction\n",
      "33.700921554565426 dB PSNR\n"
     ]
    }
   ],
   "source": [
    "batch = dataset.select([25])\n",
    "x = valid_collate_fn(batch).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(x)\n",
    "    latent = model.quantize.compand(z).round()\n",
    "    latent_reshaped = einops.rearrange(latent, 'b c (h w) -> b c h w', h=32)\n",
    "    latent_img = latent_to_pil(latent_reshaped, n_bits=8, C=4)\n",
    "    display(latent_img[0])\n",
    "    buff = io.BytesIO()\n",
    "    latent_img[0].save(buff, format='TIFF', compression='tiff_adobe_deflate')\n",
    "    tiff_bytes = buff.getbuffer()\n",
    "    print(f\"{x.numel()/len(tiff_bytes)} compression ratio\")\n",
    "    print(f\"{x.numel()/z.numel()}× dimension reduction\")\n",
    "    latent_decoded = pil_to_latent([PIL.Image.open(buff)], N=16, n_bits=8, C=4)\n",
    "    latent_decoded = einops.rearrange(latent_decoded, 'b c h w -> b c (h w)')\n",
    "    x_hat = model.decode(latent_decoded)\n",
    "    if config.post_filter:\n",
    "        x_hat = model.post_filter(x_hat)\n",
    "    x_hat = x_hat.clamp(-1,1)\n",
    "mse = torch.nn.functional.mse_loss(x,x_hat)\n",
    "PSNR = -10*mse.log10().item() + 6.02\n",
    "print(f\"{PSNR} dB PSNR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a2561-ec71-4015-8393-1eddaa202d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "play(x[0].cpu(),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c4beb-f2bb-4945-8077-e247974bb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "play(x_hat[0].cpu(),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2fed0c-4751-4871-8bef-13de93a1edea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.85600468658465 compression ratio\n",
      "23.977247495651245 dB PSNR\n"
     ]
    }
   ],
   "source": [
    "buff = io.BytesIO()\n",
    "torchaudio.save(\n",
    "    uri=buff,\n",
    "    src=x[0],\n",
    "    sample_rate=44100,\n",
    "    format='opus',\n",
    "    encoding='OPUS',\n",
    "    compression=torchaudio.io.CodecConfig(bit_rate=6000),\n",
    ")\n",
    "opus_bytes = buff.getbuffer()\n",
    "print(f\"{x.numel()/len(opus_bytes)} compression ratio\")\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.opus') as temp_file:\n",
    "    temp_file.write(opus_bytes)\n",
    "    temp_file_path = temp_file.name\n",
    "x_hat, fs2 = torchaudio.load(temp_file_path)\n",
    "x_hat = torchaudio.transforms.Resample(fs2,44100)(x_hat)\n",
    "mse1 = torch.nn.functional.mse_loss(x[0],x_hat[:,:-1])\n",
    "mse2 = torch.nn.functional.mse_loss(x[0],x_hat[:,1:])\n",
    "PSNR1 = -10*mse1.log10().item() + 6.02\n",
    "PSNR2 = -10*mse2.log10().item() + 6.02\n",
    "print(f\"{max(PSNR1,PSNR2)} dB PSNR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df6997-154f-4666-86f8-db2d37a29947",
   "metadata": {},
   "outputs": [],
   "source": [
    "play(x_hat[0].cpu(),rate=44100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
